{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wildfire-All-Resolution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EbJeT8l3AnL"
      },
      "source": [
        "# Wildfire - All Resolution Aggregation\n",
        "A previous notebook `Wildfire-5Min-Aggregation` showed how we can aggregate the fires to a 5-minute (1/12 degree) spatial resolution. However, with the climatology, there are 3 other resolutions we can deal with: 10-minute (1/6 degree), 2.5-minute (1/24 degree), and 30-second (1/120 degree). We will create fire datasets for each of these, and write them to our drive. We will also cut-down the climatology data into the bounding box we care about."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmm3Nt2H3qed"
      },
      "source": [
        "## Import Packages\n",
        "Like before, we need to install `rasterio`. We also need to mount our drive so we can access its files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19IeL8zV4PZY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "8944ac3a-7d1b-44d6-aabe-6789ef07038e"
      },
      "source": [
        "!pip install rasterio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rasterio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/81/13321f88f582a00705c5f348724728e8999136e19d6e7c56f7e6ac9bb7f9/rasterio-1.1.3-cp36-cp36m-manylinux1_x86_64.whl (18.1MB)\n",
            "\u001b[K     |████████████████████████████████| 18.1MB 196kB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from rasterio) (19.3.0)\n",
            "Collecting affine\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/a6/1a39a1ede71210e3ddaf623982b06ecfc5c5c03741ae659073159184cd3e/affine-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rasterio) (1.18.3)\n",
            "Collecting snuggs>=1.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/0e/d27d6e806d6c0d1a2cfdc5d1f088e42339a0a54a09c3343f7f81ec8947ea/snuggs-1.4.7-py3-none-any.whl\n",
            "Collecting click-plugins\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/da/824b92d9942f4e472702488857914bdd50f73021efea15b4cad9aca8ecef/click_plugins-1.1.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from rasterio) (7.1.1)\n",
            "Collecting cligj>=0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/e4/be/30a58b4b0733850280d01f8bd132591b4668ed5c7046761098d665ac2174/cligj-0.5.0-py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.6/dist-packages (from snuggs>=1.4.1->rasterio) (2.4.7)\n",
            "Installing collected packages: affine, snuggs, click-plugins, cligj, rasterio\n",
            "Successfully installed affine-2.3.0 click-plugins-1.1.1 cligj-0.5.0 rasterio-1.1.3 snuggs-1.4.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OT9ChYf4RWD"
      },
      "source": [
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "import pandas as pd\n",
        "import rasterio as rio\n",
        "from rasterio.windows import Window\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import datetime as dt\n",
        "from itertools import product\n",
        "import time\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pprint\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import sqlite3\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxD1sKdh4nP3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "56778505-4941-4fe4-d135-a3c5232674dd"
      },
      "source": [
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC6mLahT4sF1"
      },
      "source": [
        "## Info before We get Started - PLEASE READ\n",
        "Since the climatology is split up by month, we'll split up the fire data by month as well. The previous notebook had a 3-dimensional array, but being consistent with the format is important.\n",
        "\n",
        "To save the number of files we write, the fires and climatology (for each metric) will be in a *single* `.npz` file. An `.npz` file is basically a way to save multiple `numpy` arrays in a single file, along with labels. Our labels will be the 3-letter abbreviations for the months (starting with a capital letter i.e. \"Jan\", \"Feb\", ...).\n",
        "\n",
        "Unfortunately, masked arrays are not supported with this method, so instead, we save the original data (with the `no_data` value filled in) and save the mask separately in a `mask.npy` file. Since the fires and climatology are all the same resolution, we only need one mask file.\n",
        "\n",
        "The directory structure will be `Preprocessed/{Resolution}` inside our `Data` folder.\n",
        "\n",
        "To recap, each resolution folder, will have 9 files:\n",
        "\n",
        "- `fires.npz` - Data for fires for each month in \"Jan\", \"Feb\", ... labels.\n",
        "- `tmin.npz` - Minimum temperature (&deg;C)\n",
        "- `tmax.npz` - Maximum temperature (&deg;C)\n",
        "- `tavg.npz` - Average temperature (&deg;C)\n",
        "- `prec.npz` - Precipitation (mm)\n",
        "- `srad.npz` - Solar radiation (kJ m<sup>-2</sup> day<sup>-1</sup>)\n",
        "- `wind.npz` - Wind speed (m/s)\n",
        "- `vapr.npz` - Water vapor pressure (kPa)\n",
        "- `mask.npy` - The mask \n",
        "\n",
        "Let's get started! As an extra pre-caution, if you need to delete anything from drive, visit Drive directly and delete it...it's dangerous to try to delete things through code. Don't accidently delete your entire drive!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhz2t9_ND4vO"
      },
      "source": [
        "## Metadata\n",
        "General stuff to store, like filepaths, resolutions, metrics, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0fZduzmDXFE"
      },
      "source": [
        "GDRIVEROOT = '/gdrive/My Drive/Data Science and Public Policy/Final project'\n",
        "PREFIX = 'Data/Preprocessed'\n",
        "metrics = ['tmin', 'tmax', 'tavg', 'prec', \n",
        "           'srad', 'wind', 'vapr']\n",
        "resolutions = ['5m']\n",
        "divisionsPerDegree = [12]\n",
        "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
        "          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "\n",
        "# Bounding box\n",
        "LATTOP = 50\n",
        "LATBOT = 24.5\n",
        "LONLEFT = -125\n",
        "LONRIGHT = -66.5\n",
        "\n",
        "# The query that we'll run against the fire database...\n",
        "query = \"\"\"\n",
        "  SELECT FIRE_YEAR, DISCOVERY_DOY, STATE, LATITUDE, LONGITUDE, FIRE_SIZE_CLASS\n",
        "  FROM Fires\n",
        "\"\"\"\n",
        "\n",
        "# Method to convert the lat/longitude value\n",
        "# to an index corresponding to our climatology\n",
        "# data. See previous notebook for details...\n",
        "def convertToClimateIndex(value, divisionsPerDegree, direction):\n",
        "  # Start at -180, positive difference\n",
        "  if direction == 'lon':\n",
        "    if value < -180 or value >= 180:\n",
        "      raise ValueError('value was out of range.')\n",
        "    return int((value + 180) * divisionsPerDegree)\n",
        "  # Start at 90, negative difference\n",
        "  else:\n",
        "    if value < -90 or value >= 90:\n",
        "      raise ValueError('value was out of range.')\n",
        "    return int((value - 90) * divisionsPerDegree * -1)\n",
        "\n",
        "# Same method, but for converting our point-locations\n",
        "# to a fire index...\n",
        "def convertToFireIndex(value, divisionsPerDegree, direction):\n",
        "  # Start at -180, positive difference\n",
        "  if direction == 'lon':\n",
        "    if value < -125 or value >= -66.5:\n",
        "      raise ValueError('value was out of range.')\n",
        "    return int((value + 125) * divisionsPerDegree)\n",
        "  # Start at 90, negative difference\n",
        "  else:\n",
        "    if value <= 24.5 or value > 50:\n",
        "      raise ValueError('value was out of range.')\n",
        "    return int((value - 50) * divisionsPerDegree * -1)\n",
        "\n",
        "# List of tuples corresponding to\n",
        "# climatology indices...\n",
        "climateIndices = [(\n",
        "    convertToClimateIndex(LATTOP, divisions, 'lat'),\n",
        "    convertToClimateIndex(LATBOT, divisions, 'lat'),\n",
        "    convertToClimateIndex(LONLEFT, divisions, 'lon'),\n",
        "    convertToClimateIndex(LONRIGHT, divisions, 'lon')\n",
        ") for divisions in divisionsPerDegree]\n",
        "\n",
        "# Make the directories if not already present...\n",
        "for res in resolutions:\n",
        "  os.makedirs(os.path.join(GDRIVEROOT, PREFIX, res), exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvANqr5TFRnm"
      },
      "source": [
        "Now that setup is done, we can start writing files. We can technically do this in one **giant** loop, but that might be hard to digest. Instead, we'll split it up. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f2OK5XOIwPD"
      },
      "source": [
        "## Writing Climatology Files\n",
        "The function `itertools.product` allows us to do the Cartesian product of the resolution and metric like on the site. We also need the divisions per degree to convert the values to indices. However, the resolutions, divisions, and indices need to be paired up one-to-one, and so we call `zip` first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECC_0Nz6IzHb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "0dd6e2d3-033c-47df-8423-3c91f2fe7197"
      },
      "source": [
        "start = time.perf_counter()\n",
        "# By surrounding with parentheses, we can\n",
        "# unpack the variables on the fly.\n",
        "for (res, divisions, (LATTOP_index, LATBOT_index, LONLEFT_index, LONRIGHT_index)), metric in \\\n",
        "    product(zip(resolutions, divisionsPerDegree, climateIndices), metrics):\n",
        "\n",
        "  print('-{} {}'.format(res, metric.upper()))\n",
        "  localFolder = 'wc2.1_{}_{}'.format(res, metric)\n",
        "  downloadSite = 'http://biogeo.ucdavis.edu/data/worldclim/v2.1/base/{}.zip'.format(localFolder)\n",
        "  print('  - Downloading {}...'.format(localFolder + '.zip'), end='')\n",
        "  # Download zip file\n",
        "  !wget -q -N {downloadSite}\n",
        "  print('Done')\n",
        "  # Unzip to directory (-d),\n",
        "  # don't show messages (-q),\n",
        "  # and force overwrite (-o)\n",
        "  print('  - Unzipping...', end='')\n",
        "  !unzip -q -o {localFolder}.zip -d ./{localFolder}\n",
        "  print('Done')\n",
        "  # Remove the zip file\n",
        "  # !rm {localFolder}.zip\n",
        "\n",
        "  # Create a dictionary that will hold the\n",
        "  # data for each month. Eventually we will unpack\n",
        "  # this to save in a compressed npz file.\n",
        "  metricDict = {}\n",
        "  # Files are numbered 01 to 12...\n",
        "  print('  - Reading months...', end='')\n",
        "  for i, month in enumerate(months, 1):\n",
        "    tifFile = '{}_{:02}.tif'.format(localFolder, i)\n",
        "    with rio.open(os.path.join(localFolder, tifFile)) as data:\n",
        "      # Windowed reading...\n",
        "      width = LONRIGHT_index - LONLEFT_index\n",
        "      height = LATBOT_index - LATTOP_index\n",
        "      dataArray = data.read(1, window=Window(col_off=LONLEFT_index, row_off=LATTOP_index,\n",
        "                                             width=width, height=height))\n",
        "      noDataVal = data.nodata\n",
        "      dataMask = ~data.dataset_mask().astype(bool)[LATTOP_index : LATBOT_index, LONLEFT_index : LONRIGHT_index]\n",
        "    # Create masked array object...\n",
        "    # data = ma.core.MaskedArray(data=dataArray, mask=dataMask, \n",
        "    #                             fill_value=noDataVal)\n",
        "    # Subset to USA and assign it to our dictionary\n",
        "    # under the appropriate month...\n",
        "    # usaData = data[LATTOP_index : LATBOT_index, LONLEFT_index : LONRIGHT_index]\n",
        "    # usaMask = usaData.mask\n",
        "    metricDict[month] = dataArray\n",
        "  print('Done')\n",
        "  \n",
        "  # SAVE THIS METRIC!\n",
        "  print('  - Uploading {}...'.format(metric), end='')\n",
        "  np.savez(os.path.join(GDRIVEROOT, PREFIX, res, '{}.npz'.format(metric)),\n",
        "            **metricDict)\n",
        "  print('Done')\n",
        "\n",
        "  # ONLY SAVE THE MASK IF WE HAVEN'T!\n",
        "  maskFilePath = os.path.join(GDRIVEROOT, PREFIX, res, 'mask.npy')\n",
        "  # if not os.path.exists(maskFilePath):\n",
        "  print('  - Uploading mask...', end='')\n",
        "  np.save(maskFilePath, dataMask)\n",
        "  print('Done')\n",
        "\n",
        "end = time.perf_counter()\n",
        "print('FINISHED IN', end - start, 'SECONDS.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-5m TMIN\n",
            "  - Downloading wc2.1_5m_tmin.zip...Done\n",
            "  - Unzipping...Done\n",
            "  - Reading months...Done\n",
            "  - Uploading tmin...Done\n",
            "  - Uploading mask...Done\n",
            "-5m TMAX\n",
            "  - Downloading wc2.1_5m_tmax.zip...Done\n",
            "  - Unzipping...Done\n",
            "  - Reading months...Done\n",
            "  - Uploading tmax...Done\n",
            "  - Uploading mask...Done\n",
            "-5m TAVG\n",
            "  - Downloading wc2.1_5m_tavg.zip...Done\n",
            "  - Unzipping...Done\n",
            "  - Reading months...Done\n",
            "  - Uploading tavg...Done\n",
            "  - Uploading mask...Done\n",
            "-5m PREC\n",
            "  - Downloading wc2.1_5m_prec.zip...Done\n",
            "  - Unzipping...Done\n",
            "  - Reading months...Done\n",
            "  - Uploading prec...Done\n",
            "  - Uploading mask...Done\n",
            "-5m SRAD\n",
            "  - Downloading wc2.1_5m_srad.zip...Done\n",
            "  - Unzipping...Done\n",
            "  - Reading months...Done\n",
            "  - Uploading srad...Done\n",
            "  - Uploading mask...Done\n",
            "-5m WIND\n",
            "  - Downloading wc2.1_5m_wind.zip...Done\n",
            "  - Unzipping...Done\n",
            "  - Reading months...Done\n",
            "  - Uploading wind...Done\n",
            "  - Uploading mask...Done\n",
            "-5m VAPR\n",
            "  - Downloading wc2.1_5m_vapr.zip...Done\n",
            "  - Unzipping...Done\n",
            "  - Reading months...Done\n",
            "  - Uploading vapr...Done\n",
            "  - Uploading mask...Done\n",
            "FINISHED IN 31.002315985000678 SECONDS.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJDja0Svt1j5"
      },
      "source": [
        "maskFromFile = np.load(maskFilePath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIhLyjn6t6l5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "60a65324-f3ee-4dae-917d-00cd0e1072bf"
      },
      "source": [
        "np.array_equal(dataMask, maskFromFile)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-OXviAwuGn9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8374bf11-ad0a-4aaa-9af9-a91947c9195a"
      },
      "source": [
        "maskFilePath"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/gdrive/My Drive/Data Science and Public Policy/Final project/Data/Preprocessed/5m/mask.npy'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzCEnQWtegyj"
      },
      "source": [
        "## Writing Fire Files...\n",
        "Now we can write the fire files. Here, there wil only be 1 file per resolutions. In the previous notebook, we made sure that the fire data had the same shape. However, we can calculate the shape easily using the latitude longitude bounds and the number of divisions per degree. \n",
        "\n",
        "Additionally, because the query wil be the same, by separating the two loops, we only have to go through our fire database once. Each fire we will calculate the correct indices for each of the 4 resolutions, and then move onto the next fire.\n",
        "\n",
        "As a reminder, we are skipping fires in Alaska, Hawaii, and Puerto Rico.\n",
        "\n",
        "We've already written the masks in the previous loops so we can just read them directly here..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-DoL2KZlgL1"
      },
      "source": [
        "dbPrefix = 'Data/RDS-2013-0009.4_SQLITE/Data'\n",
        "dbFile = 'FPA_FOD_20170508.sqlite'\n",
        "# Establish \"connection\" to the database\n",
        "connection = sqlite3.connect(os.path.join(GDRIVEROOT, dbPrefix, dbFile))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8tFV0I6fBEo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "30c06a68-af68-4ad9-e615-00c1abda7fab"
      },
      "source": [
        "# Make a cursor...\n",
        "cursor = connection.cursor()\n",
        "\n",
        "start = time.perf_counter()\n",
        "\n",
        "# First create the 4 fire datasets...\n",
        "fireDatasets = []\n",
        "for res, divisions in zip(resolutions, divisionsPerDegree):\n",
        "  # Calculate the size of this fire dataset...\n",
        "  height = int((LATTOP - LATBOT) * divisions)\n",
        "  width = int((LONRIGHT - LONLEFT) * divisions)\n",
        "  print(width, height)\n",
        "  # Create a 3d array...\n",
        "  fireDatasets.append(np.zeros(shape=(12, height, width), dtype=np.int32))\n",
        "\n",
        "# NOW EXECUTE THE QUERY...\n",
        "# Copy-pasted from the old notebook...\n",
        "for row in cursor.execute(query):\n",
        "  # Extract information\n",
        "  year, doy, state, lat, lon, sizeClass = row\n",
        "  # If the state is Alaska (AK) or\n",
        "  # Hawaii (HI) or Puerto Rico (PR), then skip it...\n",
        "  if state in ['AK', 'HI', 'PR'] or sizeClass in ['A', 'B']:\n",
        "    continue\n",
        "  # Create a date object for the first day of the year,\n",
        "  # and add the proper number of days according to doy.\n",
        "  # These are 1-indexed so we add one less.\n",
        "  # Then, grab the month...which is also 1-indexed so we'll subtract\n",
        "  # 1 when incrementing the fire count at that location.\n",
        "  # Leap years are already taken into account with the package....\n",
        "  fireMonth = (dt.date(year, 1, 1) + dt.timedelta(days=doy - 1)).month\n",
        "\n",
        "  # Now, for each of the fire datasets,\n",
        "  # calculate the correct latitude and longitude,\n",
        "  # and increment the fire count at that location...\n",
        "  # Rows correspond to latitudes,\n",
        "  # columns correspond to longitudes...\n",
        "  for i, divisions in enumerate(divisionsPerDegree):\n",
        "    latIndex = convertToFireIndex(lat, divisions, 'lat')\n",
        "    lonIndex = convertToFireIndex(lon, divisions, 'lon')\n",
        "    # Increment the fire count!!\n",
        "    fireDatasets[i][fireMonth - 1, latIndex, lonIndex] += 1\n",
        "\n",
        "# Now, last step is to apply the same mask as\n",
        "# the climatology and remove some values,\n",
        "# then we save it a npz files...\n",
        "for i, res in enumerate(resolutions):\n",
        "  resMask = np.load(os.path.join(GDRIVEROOT, PREFIX, res, 'mask.npy'))\n",
        "  fullMask = np.asarray([np.copy(resMask) for _ in range(12)])\n",
        "  fireDatasets[i] = ma.core.MaskedArray(fireDatasets[i], mask=fullMask)\n",
        "  # Create dictionary for each month...\n",
        "  fireDict = {month: fireDatasets[i].data[j] for j, month in enumerate(months)}\n",
        "  # SAVE THE FIRE...\n",
        "  np.savez(os.path.join(GDRIVEROOT, PREFIX, res, 'firesC+.npz'), **fireDict)\n",
        "\n",
        "end = time.perf_counter()\n",
        "print('Finished in', end - start, 'seconds.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "702 306\n",
            "Finished in 8.555980020000106 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v_Q8XEmxFns",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34ea85cc-25ff-4b81-f29a-51be4b1a965f"
      },
      "source": [
        "firesAplus = np.load(os.path.join(GDRIVEROOT, PREFIX, '5m', 'fires.npz'))\n",
        "np.sum(firesAplus['Jan'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92852"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCpNbjjopcA7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e70e749b-4856-406f-80f9-44adc6f47d93"
      },
      "source": [
        "np.sum(fireDict['Jan'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16122"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRQgLWMiphhG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "90a04931-656b-48f1-8060-33d5f6d1e2fd"
      },
      "source": [
        "print(sum([np.sum(firesAplus[month]) for month in months]))\n",
        "print(sum([np.sum(fireDict[month]) for month in months]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1835646\n",
            "268042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa7GlMOmpsAb"
      },
      "source": [
        "fireAplusJan = np.copy(firesAplus['Jan'])\n",
        "fireCplusJan = np.copy(fireDict['Jan'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm5siJbDqGvS"
      },
      "source": [
        "fireAplusJan[fireAplusJan > 0] = 1\n",
        "fireCplusJan[fireCplusJan > 0] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZrucUakqqVq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "70873f89-d026-4b7a-8e6a-39757c0d1786"
      },
      "source": [
        "np.mean(fireAplusJan)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11035696329813977"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGUxdD1Jqrk7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "102696cf-22ef-4152-c545-c5f5124cb30e"
      },
      "source": [
        "np.mean(fireCplusJan)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.042735042735042736"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCZRa4z3qsW2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}