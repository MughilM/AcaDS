{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Part III - Neural Network\n",
    "Now that we have showed it is possible to fit a logistic regression model on our wildfire data, it is time to take it a step further. We will be using the same data, but this time will be implementing a neural network. The previous notebook saved the post-processed data into a CSV so we can easily read it in this notebook. Due to the relatively limited number of input variables, the network will not be very big and computational intensive. Moreover, the network structure allows for non-linear relationships to be built *between* inputs.\n",
    "\n",
    "It should be noted that this input format is not very well suited to the neural network structure, but we will press forward regardless."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27bc6be6394a0718"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Packages\n",
    "As always, we import all the packages we will use. To build the network, we will be using **Tensorflow**. However, there is one caveat here if you are using Windows. If using native Windows, then the maximum allowed Python version is 3.10. This is due to Tensorflow 2.10 being the last version **that allows for native GPU support.** If newer versions of Tensorflow wish to be used, then WSL2 need to be used instead. However, using Python 3.10 does not influence the functionality of the other required packages in any way.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a8ad2d62a703060"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# IMPORTANT, this is the same seed as the logistic regression, \n",
    "# so the same train test split is generated, and metrics are comparable.\n",
    "np.random.seed(7145)\n",
    "ROOT = '../'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T19:24:18.836523400Z",
     "start_time": "2024-02-01T19:24:07.211300400Z"
    }
   },
   "id": "89a8a955ae9e26f7",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metadata and Data Reading\n",
    "This should be familiar, but we will create variables for directory locations, and other convenient variables. Importantly, we read in the **spatialReg** CSV file that was generated during the logistic regression notebook after all the lagged variables were calculated. This will save us a lot of time and code."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1a2a13632da1c75"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  Month  fires_L1_%  tavg_L1_avg  prec_L1_avg  srad_L1_avg  wind_L1_avg  \\\n0   Jan         0.0   -17.215083         18.0  4447.666667     3.445333   \n1   Jan         0.0   -17.008937         18.5  4425.500000     3.405188   \n2   Jan         0.0   -16.799875         17.5  4429.000000     3.361625   \n3   Jan         0.0     4.225424        117.0  3243.000000     3.450847   \n4   Jan         0.0     3.318625        144.5  3287.000000     3.139708   \n\n   vapr_L1_avg  LC11_L1_%  LC12_L1_%  LC21_L1_%  ...  LC41_L5_%  LC42_L5_%  \\\n0     0.165133   0.666667        0.0        0.0  ...        0.0   0.000000   \n1     0.165156   0.750000        0.0        0.0  ...        0.0   0.000000   \n2     0.168513   1.000000        0.0        0.0  ...        0.0   0.000000   \n3     0.680000   1.000000        0.0        0.0  ...        0.0   0.000000   \n4     0.643800   0.500000        0.0        0.0  ...        0.0   0.142857   \n\n   LC43_L5_%  LC52_L5_%  LC71_L5_%  LC81_L5_%  LC82_L5_%  LC90_L5_%  \\\n0        0.0   0.000000        0.0   0.000000   0.285714   0.714286   \n1        0.0   0.000000        0.0   0.000000   0.142857   0.428571   \n2        0.0   0.000000        0.0   0.000000   0.000000   0.857143   \n3        0.0   0.000000        0.0   0.200000   0.200000   0.000000   \n4        0.0   0.142857        0.0   0.142857   0.142857   0.142857   \n\n   LC95_L5_%  fireCenter  \n0   0.000000         0.0  \n1   0.428571         0.0  \n2   0.142857         0.0  \n3   0.000000         0.0  \n4   0.000000         0.0  \n\n[5 rows x 112 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Month</th>\n      <th>fires_L1_%</th>\n      <th>tavg_L1_avg</th>\n      <th>prec_L1_avg</th>\n      <th>srad_L1_avg</th>\n      <th>wind_L1_avg</th>\n      <th>vapr_L1_avg</th>\n      <th>LC11_L1_%</th>\n      <th>LC12_L1_%</th>\n      <th>LC21_L1_%</th>\n      <th>...</th>\n      <th>LC41_L5_%</th>\n      <th>LC42_L5_%</th>\n      <th>LC43_L5_%</th>\n      <th>LC52_L5_%</th>\n      <th>LC71_L5_%</th>\n      <th>LC81_L5_%</th>\n      <th>LC82_L5_%</th>\n      <th>LC90_L5_%</th>\n      <th>LC95_L5_%</th>\n      <th>fireCenter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Jan</td>\n      <td>0.0</td>\n      <td>-17.215083</td>\n      <td>18.0</td>\n      <td>4447.666667</td>\n      <td>3.445333</td>\n      <td>0.165133</td>\n      <td>0.666667</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.285714</td>\n      <td>0.714286</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Jan</td>\n      <td>0.0</td>\n      <td>-17.008937</td>\n      <td>18.5</td>\n      <td>4425.500000</td>\n      <td>3.405188</td>\n      <td>0.165156</td>\n      <td>0.750000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.142857</td>\n      <td>0.428571</td>\n      <td>0.428571</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Jan</td>\n      <td>0.0</td>\n      <td>-16.799875</td>\n      <td>17.5</td>\n      <td>4429.000000</td>\n      <td>3.361625</td>\n      <td>0.168513</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.857143</td>\n      <td>0.142857</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Jan</td>\n      <td>0.0</td>\n      <td>4.225424</td>\n      <td>117.0</td>\n      <td>3243.000000</td>\n      <td>3.450847</td>\n      <td>0.680000</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.200000</td>\n      <td>0.200000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Jan</td>\n      <td>0.0</td>\n      <td>3.318625</td>\n      <td>144.5</td>\n      <td>3287.000000</td>\n      <td>3.139708</td>\n      <td>0.643800</td>\n      <td>0.500000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.142857</td>\n      <td>0.0</td>\n      <td>0.142857</td>\n      <td>0.0</td>\n      <td>0.142857</td>\n      <td>0.142857</td>\n      <td>0.142857</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 112 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAG = 5\n",
    "RES = '10m'\n",
    "PREFIX = os.path.join(ROOT, 'data')\n",
    "PROCESSED_PATH = os.path.join(PREFIX, 'processed', RES)\n",
    "\n",
    "spatial_reg = pd.read_csv(os.path.join(PROCESSED_PATH, 'spatialReg.csv'))\n",
    "spatial_reg.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T19:24:21.976981100Z",
     "start_time": "2024-02-01T19:24:18.838524100Z"
    }
   },
   "id": "893c7395ff2bb2b5",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing the data\n",
    "We need to follow **exactly the same processing steps** we used in the logistic regression. This involves doing class balancing of no fires vs. fires in the training set, and appropriately transforming the climatology metrics. Average temperature, solar radiation, and wind speeds are standardized using their mean and standard deviation, while the precipitation and humidity are power transformed using the Yeo-Johnson method. All standardization parameters need to calculated from the training set, and applied to the testing set. \n",
    "\n",
    "The code block is fairly long, so for additional details, please refer to the logistic regression notebook."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8aa2e2290ec5611f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New number of records: 140198\n",
      "New proportion of fires: 0.5\n"
     ]
    }
   ],
   "source": [
    "# First handle the multicollinearity.\n",
    "for lag in range(1, LAG + 1):\n",
    "    spatial_reg.drop(columns=f'LC95_L{lag}_%', inplace=True)\n",
    "# Get rid of rows with NaNs in them...\n",
    "spatial_reg = spatial_reg.dropna()\n",
    "# Do a train test split\n",
    "train_indices, test_indices = train_test_split(np.arange(spatial_reg.shape[0]), test_size=0.2)\n",
    "train_set = spatial_reg.iloc[train_indices]\n",
    "test_set = spatial_reg.iloc[test_indices]\n",
    "\n",
    "# Split the input from output\n",
    "X_train = train_set.iloc[:, 1:-1]\n",
    "y_train = train_set['fireCenter']\n",
    "X_test = test_set.iloc[:, 1:-1]\n",
    "y_test = test_set['fireCenter']\n",
    "\n",
    "# Equalize the number of positive and negative samples in the training set...\n",
    "no_fire_samples = train_set[train_set['fireCenter'] == 0]\n",
    "fire_samples = train_set[train_set['fireCenter'] == 1]\n",
    "# Randomly choose the number of 1 samples we have from the no fire samples\n",
    "chosen_no_fire_samples = no_fire_samples.sample(n=fire_samples.shape[0])\n",
    "# Concatenate both sets together, and shuffle with .sample(frac=1)\n",
    "train_set = pd.concat((chosen_no_fire_samples, fire_samples), axis=0, ignore_index=True).sample(frac=1)\n",
    "print('New number of records:', train_set.shape[0])\n",
    "print('New proportion of fires:', np.mean(train_set['fireCenter']))\n",
    "\n",
    "# Split off X and y train again\n",
    "X_train = train_set.iloc[:, 1:-1]\n",
    "y_train = train_set['fireCenter']\n",
    "\n",
    "standard_cols = [col for col in spatial_reg.columns if any(col.startswith(metric)\n",
    "                                                           for metric in ['tavg', 'srad', 'wind'])]\n",
    "power_cols = [col for col in spatial_reg.columns if any(col.startswith(metric)\n",
    "                                                        for metric in ['prec', 'vapr'])]\n",
    "\n",
    "transform = make_column_transformer(\n",
    "    (StandardScaler(), standard_cols),\n",
    "    (PowerTransformer(method='yeo-johnson'), power_cols),\n",
    "    remainder='passthrough',  # To avoid dropping columns we DON'T transform\n",
    "    verbose_feature_names_out=False\n",
    "    # To get the final mapping of input to output columns without original transformer name.\n",
    ")\n",
    "transform.fit(X_train)\n",
    "\n",
    "# Create a transformed DataFrame, with the transformed data, and the new column ordering\n",
    "X_transform = pd.DataFrame(data=transform.transform(X_train),\n",
    "                           columns=transform.get_feature_names_out(transform.feature_names_in_))\n",
    "# Now, find the new index ordering\n",
    "col_index_ordering = [X_transform.columns.get_loc(orig_col) for orig_col in X_train.columns]\n",
    "# Reindexing into the column list with the new indices will automatically reorder them!\n",
    "X_transform = X_transform[X_transform.columns[col_index_ordering]]\n",
    "\n",
    "X_test_transform = pd.DataFrame(data=transform.transform(X_test),\n",
    "                                columns=transform.get_feature_names_out(transform.feature_names_in_))\n",
    "X_test_transform = X_test_transform[X_test_transform.columns[col_index_ordering]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T19:24:26.832336200Z",
     "start_time": "2024-02-01T19:24:21.985002300Z"
    }
   },
   "id": "9e357c7613625c",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the model\n",
    "We are ready to build our network. However, because we comparably do not have that many features (~100), the network will not have too many parameters. But we can still stack a few layers to make a prediction. Starting with 105 input features, we will have 3 layers with 75, 50, and 25 output neurons respectively. The last layer will just consist of a single neuron for the binary prediction. We will use ReLU for intermediate layer activations, and sigmoid for the final layer, since the output is required to be between 0 and 1."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb301c2b7125929f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 75)                7950      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                3800      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 25)                1275      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 26        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,051\n",
      "Trainable params: 13,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "in_features = X_transform.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(75, activation='relu', input_shape=(in_features,)))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  # Output!\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T19:24:31.408358200Z",
     "start_time": "2024-02-01T19:24:26.825775800Z"
    }
   },
   "id": "240cd0b127916f2f",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training!\n",
    "For training, we simply pass in our transformed training, and provide the transformed test data as \"validation\". This way we can see the accuracy performance as it trains. We will use the standard Adam optimizer, and the binary cross entropy loss function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d12c0172d66d250"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2191/2191 [==============================] - 12s 5ms/step - loss: 0.4129 - acc: 0.8122 - val_loss: 0.4090 - val_acc: 0.8127\n",
      "Epoch 2/30\n",
      "2191/2191 [==============================] - 12s 5ms/step - loss: 0.4034 - acc: 0.8165 - val_loss: 0.4069 - val_acc: 0.8139\n",
      "Epoch 3/30\n",
      "2191/2191 [==============================] - 12s 5ms/step - loss: 0.4014 - acc: 0.8172 - val_loss: 0.4147 - val_acc: 0.8102\n",
      "Epoch 4/30\n",
      "2191/2191 [==============================] - 10s 4ms/step - loss: 0.4004 - acc: 0.8176 - val_loss: 0.4125 - val_acc: 0.8127\n",
      "Epoch 5/30\n",
      "2191/2191 [==============================] - 11s 5ms/step - loss: 0.3991 - acc: 0.8183 - val_loss: 0.4064 - val_acc: 0.8131\n",
      "Epoch 6/30\n",
      "2191/2191 [==============================] - 12s 5ms/step - loss: 0.3983 - acc: 0.8183 - val_loss: 0.4068 - val_acc: 0.8127\n",
      "Epoch 7/30\n",
      "2191/2191 [==============================] - 16s 7ms/step - loss: 0.3977 - acc: 0.8191 - val_loss: 0.3930 - val_acc: 0.8220\n",
      "Epoch 8/30\n",
      "2191/2191 [==============================] - 13s 6ms/step - loss: 0.3971 - acc: 0.8196 - val_loss: 0.3934 - val_acc: 0.8196\n",
      "Epoch 9/30\n",
      "2191/2191 [==============================] - 13s 6ms/step - loss: 0.3963 - acc: 0.8194 - val_loss: 0.3821 - val_acc: 0.8280\n",
      "Epoch 10/30\n",
      "2191/2191 [==============================] - 13s 6ms/step - loss: 0.3951 - acc: 0.8199 - val_loss: 0.4189 - val_acc: 0.8113\n",
      "Epoch 11/30\n",
      "2191/2191 [==============================] - 12s 6ms/step - loss: 0.3951 - acc: 0.8193 - val_loss: 0.3979 - val_acc: 0.8223\n",
      "Epoch 12/30\n",
      "2191/2191 [==============================] - 10s 5ms/step - loss: 0.3940 - acc: 0.8201 - val_loss: 0.4060 - val_acc: 0.8117\n",
      "Epoch 13/30\n",
      "2191/2191 [==============================] - 12s 6ms/step - loss: 0.3937 - acc: 0.8205 - val_loss: 0.4111 - val_acc: 0.8148\n",
      "Epoch 14/30\n",
      "2191/2191 [==============================] - 14s 6ms/step - loss: 0.3930 - acc: 0.8209 - val_loss: 0.4011 - val_acc: 0.8126\n",
      "Epoch 15/30\n",
      "2191/2191 [==============================] - 13s 6ms/step - loss: 0.3923 - acc: 0.8216 - val_loss: 0.4089 - val_acc: 0.8135\n",
      "Epoch 16/30\n",
      "2191/2191 [==============================] - 13s 6ms/step - loss: 0.3921 - acc: 0.8214 - val_loss: 0.4100 - val_acc: 0.8118\n",
      "Epoch 17/30\n",
      "2191/2191 [==============================] - 13s 6ms/step - loss: 0.3911 - acc: 0.8217 - val_loss: 0.3934 - val_acc: 0.8256\n",
      "Epoch 18/30\n",
      "2191/2191 [==============================] - 11s 5ms/step - loss: 0.3905 - acc: 0.8214 - val_loss: 0.4034 - val_acc: 0.8118\n",
      "Epoch 19/30\n",
      "2191/2191 [==============================] - 11s 5ms/step - loss: 0.3900 - acc: 0.8222 - val_loss: 0.4034 - val_acc: 0.8118\n",
      "Epoch 20/30\n",
      "2191/2191 [==============================] - 11s 5ms/step - loss: 0.3895 - acc: 0.8226 - val_loss: 0.3995 - val_acc: 0.8202\n",
      "Epoch 21/30\n",
      "2191/2191 [==============================] - 11s 5ms/step - loss: 0.3886 - acc: 0.8232 - val_loss: 0.4115 - val_acc: 0.8081\n",
      "Epoch 22/30\n",
      "2191/2191 [==============================] - 11s 5ms/step - loss: 0.3880 - acc: 0.8234 - val_loss: 0.4021 - val_acc: 0.8112\n",
      "Epoch 23/30\n",
      "2191/2191 [==============================] - 11s 5ms/step - loss: 0.3876 - acc: 0.8235 - val_loss: 0.4018 - val_acc: 0.8166\n",
      "Epoch 24/30\n",
      "2191/2191 [==============================] - 11s 5ms/step - loss: 0.3864 - acc: 0.8245 - val_loss: 0.4037 - val_acc: 0.8117\n",
      "Epoch 25/30\n",
      "2191/2191 [==============================] - 11s 5ms/step - loss: 0.3855 - acc: 0.8244 - val_loss: 0.4050 - val_acc: 0.8112\n",
      "Epoch 26/30\n",
      "2191/2191 [==============================] - 11s 5ms/step - loss: 0.3845 - acc: 0.8256 - val_loss: 0.4113 - val_acc: 0.8151\n",
      "Epoch 27/30\n",
      "2191/2191 [==============================] - 11s 5ms/step - loss: 0.3844 - acc: 0.8253 - val_loss: 0.3950 - val_acc: 0.8144\n",
      "Epoch 28/30\n",
      "2191/2191 [==============================] - 11s 5ms/step - loss: 0.3832 - acc: 0.8260 - val_loss: 0.4182 - val_acc: 0.8119\n",
      "Epoch 29/30\n",
      "2191/2191 [==============================] - 11s 5ms/step - loss: 0.3825 - acc: 0.8263 - val_loss: 0.3987 - val_acc: 0.8138\n",
      "Epoch 30/30\n",
      "2191/2191 [==============================] - 11s 5ms/step - loss: 0.3819 - acc: 0.8271 - val_loss: 0.4046 - val_acc: 0.8126\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1db91442f20>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_transform, y_train, batch_size=64, epochs=30, validation_data=(X_test_transform, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T19:30:27.409703400Z",
     "start_time": "2024-02-01T19:24:31.403354200Z"
    }
   },
   "id": "5e45463d292be0c7",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "On paper, the accuracy is 81%, but in our case, the accuracy doesn't tell the story. We'll show a classification report just like the logistic regression.\n",
    "## Evaluation\n",
    "The evaluation process is straightforward, we grab the predictions with `model.predict`, and have our threshold be 0.5. Any probabilities greater than this amount, and the model predicts a fire, and anything less, no fire. This is the threshold that the logisiic regression operated under, so we'll do as close as a comparison as we can."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1988474ae8b4aa84"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2289/2289 [==============================] - 4s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.87     55761\n",
      "           1       0.58      0.82      0.67     17475\n",
      "\n",
      "    accuracy                           0.81     73236\n",
      "   macro avg       0.75      0.81      0.77     73236\n",
      "weighted avg       0.85      0.81      0.82     73236\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "predictions = model.predict(X_test_transform).ravel()\n",
    "predictions[predictions < threshold] = 0\n",
    "predictions[predictions >= threshold] = 1\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T19:56:47.111783100Z",
     "start_time": "2024-02-01T19:56:41.371502900Z"
    }
   },
   "id": "240f4e6603c74602",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compared with our logistic regression, our metrics are about 1-2 points lower across the board. Perhaps the fully connected neural network did not work as well as we thought. There are other neural networks we can try.\n",
    "## Conclusion\n",
    "This relatively small post went over how to build a neural network solution to our wildfire incidence problem. This network is extremely barebones, as it simply took our input data and fed it directly through a fully connected network. To extract even more performance, different processing to the dataset will need to be performed.\n",
    "\n",
    "It might be tempting to apply an image recognition CNN network to the fire, climatology, and land usage images themselves, but there could be reasons why a CNN may not be viable. Each pixel in the image is treated as both an input **and** output. Conventional CNNs produce a single label given a single image. However, here, we are basically classifying each pixel in the image. \n",
    "\n",
    "The particular class of image problems is called **image segmentation**, which requires an entirely different network architecture. However, the downside is that comparably, we do not have that many images to work with, since the original climatology images were only separated by month. Additional data would need to be collected at a more fine temporal resolution.\n",
    "\n",
    "Neural networks should only be used as a last resort to most machine learning problems. In fact, we did not even test very basic solutions. For example, we could have simply assigned fire incidence as such: If a majority of neighboring cells had a fire, then the central spot had a fire. This does not require any training, nor does it require the climatology or land usage data. This could also easily be tested at various lag values.\n",
    "\n",
    "So it is important to stay grounded in which solution you end up choosing, ensuring it is not overly complicated, and that the algorithm is not being forced to solve the problem."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc3ae1cadaae3504"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
