# Academic Data Science Projects
This repository differs from my other Data Science repository in that this one only contains project that I did as part of my academic, including undergraduate and graduate. 

## Graduate Courses
- **COMS4995** - Applied Deep Learning
    - **Assignment 1**: Creating and training a linear model on MNIST with no hidden layers. This includes plotting loss and accuracy. We eventually transition to a deep neural network. As a bonus, we use `tf.data` and `GradientTape` to batch the data into a TensorFlow Dataset object instead of using the bulit-in data pipelining in `model.fit`. Lastly, we actually visualize what the learned weights look like in our trained model, and see if there are any patterns present.
    - **Assignment 2**: We build a deep classifier for images of flowers. However, we actually **deploy** this model onto the web using TensorFlow.js, where users can submit images and see predictions in real-time. As a second part to this assignment, we build our own small dataset of landmarks around campus and use transfer learning to achieve a satisfactory accuracy on this small set. This model is also deployed onto the web at [this site](http://mughilm.github.io/COMS4995-ADL/).
    - **Assignment 3**: We learn to utilize some external tools to help us analyze model performance. One is TensorBoard to show training metrics. We showcase this by examining the difference in performance between two activation functions, ReLU and Swish. The swish function was shown to have better consistent performance. Next, we apply the LIME interpretability technique on the Inception image classifier to see which regions of an image contributed to the prediction. Lastly, we show the use of Keras Tuner to tune the hyperparameters of a small model.
    - **Assignment 4**: This notebook contains the implementation of the visual question-answering task, where the model is presented with an image and a short yes/no question, and the model must answer yes/no to the question using the context from the image. The dataset is the COCO 2014 VQA dataset. The first part goes through all of the preprocessing steps. They are shown in the notebook itself, even though the instructions recommend to create a separate one. We use Inception V3 model as a starting point for the images. To model the qusetions, we pad/truncate the questions to 13 words long, and pass them through a basic LSTM network. Along the way, we do multiple sanity checks to make sure our preprocessing steps are working as intended. We also store the activations of the images to disk, so they do not have to be fed through the model again and again.
- **INAF6506** - Data Science and Public Policy - The projects and reports in this class were **done in substantial collaboration with 3 other students:** Max Mauerman, Priyanka Sethy, and Pei Yin Teo.
    - **Local Level Crime**: Given beat-level crime in Chicago and Cook County, we created visualizations to better understand the distributions of crime around the city. Mulitple theories were tested to see if they held with the visualizations. One is the broken-windows theory, which states the visibility of minor crimes (burglaries, civil disorder) give rise to more violet crimes, such as murders. Using this theory, some police departments try to crack down on minor crimes, hoping to deter major crimes. We test this theory among others, by looking at correlations and crime amounts on a time lag scale.
    - **Rare events**: Extremely rare events, such as weather events, are difficult to predict by definition. With the onset of climate change and global warming, rare weather events will become more common. With this in mind, we examined hurricanes from the 1850s until today with factors such as wind speeds, temperature, pressure, etc. We saw if there were differences in wind speeds that may explain the increase in hurricane frequency (storms such as Harvey, Sandy). As a final exercise, we attempted to **reconstruct** the paths the hurricanes take using wind speed and location by applying K-means clustering on this data.
    - **Final project: Wildfire prediction**: For the final project in this class, we were given free reign. We chose to examine if we could ecological factors and land use to predict the incidence of wildfires. The data we had were all recorded wildfires from 1992 to 2015 across the US, which included discovery date, approximate size, and location. We also utilized climatology data and land use classification i.e. what each sector of land is being used for (industrial, farmland, etc.). We also analyzed data from California separately. The model was a _spatial_ logistical regression which took inputs not only from that moment in time, but of the areas and blocks from around the center point.
